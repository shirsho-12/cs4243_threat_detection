{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: ok\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreatDataset(Dataset):\n",
    "    def __init__(self, data, loader_type='train', transforms = None):\n",
    "        self.folder_names = ['carrying', 'threat', 'normal']\n",
    "        self.data = data\n",
    "        self.transform = transforms\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data[idx]\n",
    "        label = self.folder_names.index(data.parent.name)\n",
    "        image = cv2.imread(str(data))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "def split_data(data_dir, train_size=0.8, val_size = 0.1):\n",
    "    random.seed(1234)\n",
    "    data = Path('data').glob('*/*')\n",
    "    data = [x for x in data if x.is_file() and x.suffix != '.zip']\n",
    "    random.shuffle(data)\n",
    "    train_size = int(len(data) * train_size)\n",
    "    val_size = int(len(data) * val_size)\n",
    "    train_data = data[:train_size]\n",
    "    val_data = data[train_size:train_size+val_size]\n",
    "    test_data = data[train_size+val_size:]\n",
    "\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "\n",
    "train_data, val_data, test_data = split_data('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transforms = {\n",
    "    'train': train_transforms,\n",
    "    'val': val_transforms,\n",
    "    'test': test_transforms\n",
    "}\n",
    "\n",
    "train_dataset = ThreatDataset(train_data, loader_type=\"train\", transforms=train_transforms)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts import dataloader\n",
    "\n",
    "train_loader, val_loader, test_loader = dataloader.get_loaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 2, 1, 1, 0, 2, 1, 2, 0, 2, 1, 2,\n",
       "        1, 1, 2, 0, 0, 1, 1, 2])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = iter(train_loader).next()\n",
    "torch.argmax(F.one_hot(y, num_classes=3), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For NLLLoss, we need to use the logit distribution\n",
    "# https://discuss.pytorch.org/t/what-is-the-difference-between-nllloss-and-crossentropyloss/15553\n",
    "# The CrossEntropyLoss combines the LogSoftmax and NLLLoss in one single class\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, criterion, scheduler, device):\n",
    "        # The trainer uses a one-hot distribution for the labels, so we need to use the CrossEntropyLoss\n",
    "        # instead of the NLLLoss\n",
    "        # Using FCC layer as the last layer, we can try to use basic loss functions like MSE or L1\n",
    "        \n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.scheduler = scheduler\n",
    "        self.best_acc = 0.5\n",
    "        if (device == 'cuda') and torch.cuda.is_available():\n",
    "            self.device = torch.device('cuda')\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "\n",
    "    def train(self, train_loader, val_loader, epochs=10):\n",
    "        self.model.to(self.device)\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"EPOCH {epoch}\")\n",
    "            self.model.train()\n",
    "            tq = tqdm(enumerate(train_loader))\n",
    "            for i, (x, y) in tq:\n",
    "                x = x.to(self.device)\n",
    "                y_label = y\n",
    "                y = F.one_hot(y, num_classes=3).to(self.device).float()\n",
    "                total += y.size(0)\n",
    "                self.optimizer.zero_grad()\n",
    "                y_pred = self.model(x)\n",
    "                loss = self.criterion(y_pred, y)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                # Calculate Accuracy - Only for softmax/logit distributions  \n",
    "                _, predicted = torch.max(y_pred.data, 1)\n",
    "                correct += (predicted.cpu() == y_label).sum().item()\n",
    "                tq.set_postfix(loss=loss.item(), acc=correct/total)\n",
    "                if i % 100 == 0:\n",
    "                    print(f'Epoch: {epoch}, Loss: {loss.item()}')\n",
    "            self.validate(val_loader)\n",
    "            self.scheduler.step()\n",
    "            print(f'Epoch: {epoch}, Accuracy: {correct/total}')\n",
    "\n",
    "    def validate(self, val_loader):\n",
    "        self.model.eval()\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            tq = tqdm(enumerate(val_loader))\n",
    "            for i, (x, y) in tq:\n",
    "                x = x.to(self.device)\n",
    "                y_label = y\n",
    "                y = F.one_hot(y, num_classes=3).to(self.device).float()\n",
    "                y_pred = self.model(x)\n",
    "                loss = self.criterion(y_pred, y)\n",
    "\n",
    "                total += y.size(0)\n",
    "                _, predicted = torch.max(y_pred.data, 1)\n",
    "                # print(predicted)\n",
    "                \n",
    "                correct += (predicted.cpu() == y_label).sum().item()\n",
    "                if i % 100 == 0:\n",
    "                    print(f'Validation Loss: {loss.item()}')\n",
    "            print(f'Validation Accuracy: {correct/total}')\n",
    "            if correct/total > self.best_acc:\n",
    "                self.best_acc = correct/total\n",
    "                print('Saving model...')\n",
    "                torch.save(self.model.state_dict(), 'best_model.pth')\n",
    "    \n",
    "    def test(self, test_loader):\n",
    "        self.model.eval()\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for i, (x, y) in tqdm(enumerate(test_loader)):\n",
    "                x = x.to(self.device)\n",
    "                y_label = y\n",
    "                y = F.one_hot(y, num_classes=3).to(self.device).float()\n",
    "                total += y.size(0)\n",
    "                y_pred = self.model(x)\n",
    "                loss = self.criterion(y_pred, y)\n",
    "\n",
    "                _, predicted = torch.max(y_pred.data, 1)\n",
    "                correct += (predicted.cpu() == y_label).sum().item()\n",
    "                if i % 100 == 0:\n",
    "                    print(f'Test Loss: {loss.item()}')\n",
    "        print(f'Accuracy: {100 * correct / total}')\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        torch.save(self.model.state_dict(), path)      \n",
    "\n",
    "    def load_model(self, path):\n",
    "        self.model.load_state_dict(torch.load(path)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18, resnet101\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "# from torch import nn\n",
    "import torch.optim as optim \n",
    "\n",
    "model = resnet18(pretrained=False)\n",
    "model.fc = nn.Linear(512, 3)\n",
    "\n",
    "model.load_state_dict(torch.load('best_model_25.pth',  map_location=torch.device('cpu')))\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "# trainer = Trainer(model, optimizer, criterion, scheduler, 'cuda')\n",
    "# trainer.train(train_loader, val_loader, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11178051"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Res101Model(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.model = resnet101(pretrained=True)\n",
    "        self.model.fc = nn.Linear(2048, 512)\n",
    "        self.fc_2 = nn.Linear(512, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = self.fc_2(x)\n",
    "        return x   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import efficientnet_b2\n",
    "\n",
    "class EffNetModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.model = efficientnet_b2(pretrained=True)\n",
    "        self.model._fc = nn.Linear(1408, 512)\n",
    "        self.fc_2 = nn.Linear(512, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = self.fc_2(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1440, 1080, 3)\n",
      "(1440, 1080)\n",
      "(1440, 1080, 3)\n"
     ]
    }
   ],
   "source": [
    "import cv2 \n",
    "from pathlib import Path \n",
    "import numpy as np\n",
    "\n",
    "img = Path.cwd() / 'data' / 'normal' / '00000.42797_30.png'\n",
    "img = cv2.imread(str(img))\n",
    "print(img.shape)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "print(img.shape)\n",
    "# img = cv2.resize(img, (224, 224))\n",
    "img = np.dstack((img, img, img))\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('Vision')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fa38dedfb6664aa993a6ecbbad1c80b49362dc7a9b41f4096b0f17f364f944d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
